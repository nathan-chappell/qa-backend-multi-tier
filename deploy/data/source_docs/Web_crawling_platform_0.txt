our Web crawling platform.
One of our clients is in a business of internet crawling and data mining. They need to collect millions of web pages from hundreds of different web sites and extract relevant parts of data from them every month. The number of pages that needs to be retrieved and analyzed varies from small (several hundred) to huge (several million) per web site. Some of the sites are crawled recurringly and some just once, but sometimes under a very strict deadline. The original approach for meeting such demands included development of a dedicated app for each of the target web sites. Initially, it did not rely on any of the modern cloud platforms, so there was a need to deal with a large payload by splitting it and running multiple instances of the same app on many different machines, each processing its own part. Over time, the maintenance of all those apps became difficult from both development and maintenance standpoints. One had to deal with constant changes in the structure of crawled web sites, updating apps to keep track with technology trends, writing new apps in the most efficient and optimized way and finally, constantly reconfiguring how and where they run. 
That all merged into an idea of developing a highly scalable generic solution to run any kind of web crawling process - actually any kind of workflow - so that processing, transforming and storing of the collected data is also covered. Additionally, we had to improve the crawling logic for some of the problematic web sites which required constant workflow changes and manual interaction due to their advanced bot protection. Finally, we had to find a way for the new solution to be flexible to the constantly changing load and deadline demands.

